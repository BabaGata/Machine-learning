function [theta, J_history] = gradientDescentMulti(X, y, theta, alpha, num_iters)m = length(y); % number of training examplesJ_history = zeros(num_iters, 1);for iter = 1:num_iters    a = (X * theta) - y;    b = sum(a .* X) * (alpha / m);    theta = theta - b';    % Save the cost J in every iteration        J_history(iter) = computeCostMulti(X, y, theta);endend